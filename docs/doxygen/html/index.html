<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=9"/>
<meta name="generator" content="Doxygen 1.8.13"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Grasp Planner: Main Page</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr style="height: 56px;">
  <td id="projectalign" style="padding-left: 0.5em;">
   <div id="projectname">Grasp Planner
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.8.13 -->
<script type="text/javascript">
var searchBox = new SearchBox("searchBox", "search",false,'Search');
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
</script>
<div id="main-nav"></div>
</div><!-- top -->
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<iframe src="javascript:void(0)" frameborder="0" 
        name="MSearchResults" id="MSearchResults">
</iframe>
</div>

<div class="header">
  <div class="headertitle">
<div class="title">Grasp Planner Documentation</div>  </div>
</div><!--header-->
<div class="contents">
<div class="textblock"><h3>This ROS2 package provides an <b>algorithmic, depth based approach</b> to generate a <b>3+1 Degree of Freedom (DOF) Grasp pose</b> from a depth image.</h3>
<p><a href="https://travis-ci.com/gtan039/algorithmic-depth-based-grasp-planner"></a></p>
<h1>Introduction</h1>
<p>Traditionally, grasp pose generation is done through using convolutional neural networks (CNNs) to achieve grasp plans. The issues with using machine learning and neural networks is several fold</p>
<ol type="1">
<li>Computation power required for fast grasp pose planning using CNNs.</li>
<li>Dataset for training neural netwoks are currently restricted to 2 finger grippers (notably, the <a href="http://pr.cs.cornell.edu/grasping/rect_data/data.php">Cornell Grasping Dataset</a> has been the most comprehensive and well labelled dataset for current grasp planning neural networks)</li>
<li>Training of new types of grippers require <b>manual labelling</b> of new datasets (labour intensive).</li>
<li>Accurate and stable grasp poses may not be available for <b>irregular objects</b>, so specifically labelled dataset is needed in order to generate accurate grasps</li>
</ol>
<h2>Package example</h2>
<p>This ROS2 package presents a solution that requires no training, no labelling and little computational power to generate a 3 + 1 DOF grasp poses. The modular design of this package also allows for expansion into other gripper types. Current support for this package includes <b>2 finger gripper and single suction cup gripper</b></p>
<p><br />
</p>
<h1>Package Details</h1>
<h2>Assumptions</h2>
<h3>Grasp object poses</h3>
<p>This package assumes a <b>top down</b> camera set up overlooking the grasping area. Assuming that the axis through the image plane is the z axis, grasp poses will consider a <b>one dimensional change in orientation</b> in the z-direction (resulting in a 3+1 DOF grasp pose)</p>
<h3>Image Quality</h3>
<p>This package was tested using an input depth image from the Intel Realsense D415 RGB-D camera. Certain variations may occur if a different camera is used</p>
<h3>Work Surface</h3>
<p>This package assumes that the object is placed on a relatively flat surface. As this package requires the use of the depth image from the camera above the work surface, we assume that the distance from the work surface to the camera is constant.</p>
<h3>End Effector Support</h3>
<p>This package currently supports a 2 Finger gripper and a single suction cup gripper. Further development will include multiple finger gripper and suction cup array support.</p>
<h2>Package subscriber</h2>
<p>This package consists of a subscriber that subscribes to the following topic with the message structure shown</p>
<p>Topic Name : <code>/perception_output</code></p>
<p>Message Name: RectOutput.msg</p>
<table class="doxtable">
<tr>
<th>Message name </th><th>Field Type </th><th>Explanation  </th></tr>
<tr>
<td>header </td><td>std_msgs/Header </td><td>General information from the camera </td></tr>
<tr>
<td>objects </td><td>DlObject[] </td><td>Information about the object (refer below to the DlOBject message type. </td></tr>
<tr>
<td>frame_width </td><td>uint32 </td><td>Width of the depth image </td></tr>
<tr>
<td>frame_height </td><td>uint32 </td><td>Height of the depth image </td></tr>
<tr>
<td>num_objects </td><td>uint32 </td><td>Number of objects in scene </td></tr>
<tr>
<td>depth_image </td><td>sensor_msgs/Image </td><td>Depth image of the work area </td></tr>
<tr>
<td>camera_info </td><td>sensor_msgs/CameraInfo </td><td>Camera-specific information </td></tr>
<tr>
<td>roi_array </td><td>sensor_msgs/RegionOfInterest[] </td><td>Array of bounding boxes containing the objects </td></tr>
</table>
<p>Message Name: DlObject.msg</p>
<table class="doxtable">
<tr>
<th>Message name </th><th>Field Type </th><th>Explanation  </th></tr>
<tr>
<td>name </td><td>string </td><td>Name of object </td></tr>
<tr>
<td>pos </td><td>geometry_msgs/PoseStamped </td><td>Pose of object </td></tr>
<tr>
<td>roi </td><td>sensor_msgs/RegionOfInterest </td><td>Bounding Box for the object </td></tr>
<tr>
<td>breadth </td><td>float64 </td><td>Real object breadth </td></tr>
<tr>
<td>length </td><td>float64 </td><td>Real object breadth </td></tr>
<tr>
<td>height </td><td>float64 </td><td>Real object height </td></tr>
</table>
<h2>Package Publisher</h2>
<p>This package consists of a publisher that publishes to the following topic with the message structure shown</p>
<p>Topic Name : <code>/grasp_poses</code></p>
<table class="doxtable">
<tr>
<th>Message name </th><th>Field Type </th><th>Explanation  </th></tr>
<tr>
<td>num_objects </td><td>uint32 </td><td>Number of grasp objects in the scene </td></tr>
<tr>
<td>grasp_poses </td><td>geometry_msgs/PoseStamped[] </td><td>Array of grasp object poses </td></tr>
<tr>
<td>object_poses </td><td>geometry_msgs/PoseStamped[] </td><td>Array of grasp poses </td></tr>
<tr>
<td>object_shapes </td><td>shape_msgs/SolidPrimitive[] </td><td>Array of object shapes (Used to create collision objects for path planning) </td></tr>
</table>
<p><br />
 </p><hr/>
<h2>Configuring grasp attributes</h2>
<p>In order for the grasp planner to plan the right type of grasp, we need to first create a configuration file in the config folder , <code>attributes.yaml</code>. It is advised to write over the current <code>attributes.yaml</code> file to prevent any YAML parsing errors</p>
<h3>Finger Gripper</h3>
<h4>fingers</h4>
<p>Number of fingers for the end effector. <b>currently only 2 fingered grippers are supported</b> </p><h4>distance_between_fingers</h4>
<p>The distance between the fingers of the end effector(in mm). Using the Robotiq 2F-85 gripper as an example:</p>
<h4>longest_gripper_dim</h4>
<p>The longest dimensions of a finger (in mm). Using the robotiq 2F-85 gripper:</p>
<h4>table_height</h4>
<p>The distance between the camera used to capture the workspace and the surface on which the object is on.</p>
<h3>Suction Gripper</h3>
<h4>length_cups</h4>
<p>The number of cups in the length dimension. <b>Currently only support value of 1</b> </p><h4>breadth_cups</h4>
<p>The number of cups in the breadth dimension. <b>Currently only support value of 1</b> </p><h4>radius</h4>
<p>The radius one of the suction cup. </p><h4>table_height</h4>
<p>The distance between the camera used to capture the workspace and the surface on which the object is on. </p><hr/>
<h1>How the package works</h1>
<p>This package uses information from the depth image captured by the camera to generate grasp samples, and the quality of the grasps will be determined by a Grasp Decide Index (GDI).</p>
<p>This concept was demonstrated in <a href="https://arxiv.org/pdf/2001.05856.pdf">this paper</a>, with changes in the method of sampling potential points, due to the abstraction of the perception component of the system to a separate perception package. There is also an additonal support for single suction cup grippers.</p>
<p>Given that a two finger gripper and a single suction cup gripper are considered to be the "base cases" for a finger gripper and a suction cup array, this provides opportunities for expansion to more complicated grippers beyond the scope of the paper. </p>
</div></div><!-- contents -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by &#160;<a href="http://www.doxygen.org/index.html">
<img class="footer" src="doxygen.png" alt="doxygen"/>
</a> 1.8.13
</small></address>
</body>
</html>
